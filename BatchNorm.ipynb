{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731bc1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%cd ..\n",
    "%run Untitled.ipynb\n",
    "#%cd BestModels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66ead4",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Download the dataset, divide it in train, validation and test sets and then define the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1958786b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest_data = fetch_covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f7040c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest_key = list(forest_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1a2c818",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df5e9d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([581012, 54]), (581012,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(forest_data['data']).shape, forest_data['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38f78621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_forest = 32\n",
    "\n",
    "# Split train-validation-test sets \n",
    "forest_x_train, forest_x_test, forest_y_train, forest_y_test = train_test_split(forest_data['data'], forest_data['target'], \n",
    "                                                                                test_size = 0.2, shuffle = True, random_state=1234)\n",
    "forest_x_train, forest_x_valid, forest_y_train, forest_y_valid = train_test_split(forest_x_train, forest_y_train, \n",
    "                                                                                test_size = 0.25, shuffle = True, random_state=1234)\n",
    "# Create datasets, I don't drop the last in train set cause is of 30 instead of 32\n",
    "forest_train = DataLoader(MyDataSet(forest_x_train, forest_y_train, forest_data), batch_size = batch_forest, shuffle = True)\n",
    "forest_valid = DataLoader(MyDataSet(forest_x_valid, forest_y_valid, forest_data), batch_size = batch_forest, shuffle = True)\n",
    "forest_test = DataLoader(MyDataSet(forest_x_test, forest_y_test, forest_data), batch_size = batch_forest, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb4dc1",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Here we fix the weight initialization and the random weights that we will need for a network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a83cc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest_net = MLP_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "978e0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION STUFF FOR THE NETWORKS\n",
    "\n",
    "# Save the initial weight configuration that will be used to train all the networks\n",
    "torch.save(forest_net.state_dict(), 'weight_initialization.pth')\n",
    "\n",
    "# Select the random weights to train and save their index in the various linear layer\n",
    "#Create ad array to store the 2 index of each weight \n",
    "memory = np.zeros((2040,2), dtype=np.int_)#3))\n",
    "\n",
    "#Select the random weights which are trainable\n",
    "rand_weight = np.random.randint(0,232990, size = 2040, dtype = np.int_)\n",
    "\n",
    "res = 0\n",
    "layers = ['1','2','3','4','out']\n",
    "dim_layer = [(500,54), (300,500), (150,300), (70,150), (7,70)]\n",
    "num_weight_lin = []\n",
    "\n",
    "#Find the indexes that identify the selected weights \n",
    "for i in range(5):\n",
    "    dim1,dim2 = dim_layer[i]\n",
    "    mask = np.clip(rand_weight-(dim1*dim2), a_min = None,a_max = 0).astype(np.bool_) # mask to select the weights belonging to a layer\n",
    "    layer_weight = rand_weight[mask] # Select the weight that belong to that layer\n",
    "    num_weight_lin.append(len(layer_weight))\n",
    "    \n",
    "    # save their index position\n",
    "    memory[res : res+num_weight_lin[-1], 0] = layer_weight//dim2\n",
    "    memory[res : res+num_weight_lin[-1], 1] = layer_weight%dim2\n",
    "\n",
    "    #Delete the weights belonging to the first linear layer\n",
    "    rand_weight = rand_weight[(mask-1).astype(np.bool_)] -(dim1*dim2)\n",
    "\n",
    "    #Save the last weight entrance that we update\n",
    "    res += num_weight_lin[-1]\n",
    "\n",
    "torch.save((memory, num_weight_lin), 'memory.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d85a9",
   "metadata": {},
   "source": [
    "# Architecture \n",
    "The architecutre is the same for all the networks but the trainable weights change. Actually the linear layer in the random weights network architecture are costum, but the only difference that for the random trainable parameters we add a weight that is trainable while the typical weight matrix of the pytorch linear layer is freezed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dc0dc",
   "metadata": {},
   "source": [
    "## All parameters are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_net = MLP_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45813fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin1.weight True\n",
      "lin2.weight True\n",
      "lin3.weight True\n",
      "lin4.weight True\n",
      "linout.weight True\n",
      "bn1.weight True\n",
      "bn1.bias True\n",
      "bn1.running_mean True\n",
      "bn1.running_var True\n",
      "bn1.num_batches_tracked True\n",
      "bn2.weight True\n",
      "bn2.bias True\n",
      "bn2.running_mean True\n",
      "bn2.running_var True\n",
      "bn2.num_batches_tracked True\n",
      "bn3.weight True\n",
      "bn3.bias True\n",
      "bn3.running_mean True\n",
      "bn3.running_var True\n",
      "bn3.num_batches_tracked True\n",
      "bn4.weight True\n",
      "bn4.bias True\n",
      "bn4.running_mean True\n",
      "bn4.running_var True\n",
      "bn4.num_batches_tracked True\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights and check if it is done correctly\n",
    "InitializeWeight(forest_net)\n",
    "HaveSameWeight(forest_net, 'weight_initialization.pth', 'cpu', is_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a394cf9-20c7-4169-b44d-55ca7d4580ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check that all the paramters are trainable \n",
    "for param in forest_net.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2330f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "rand = False # Set to True when we train the random weights network\n",
    "forest_len_val = forest_y_valid.shape[0]\n",
    "forest_len_train = forest_y_train.shape[0]\n",
    "forest_len_test = forest_y_test.shape[0]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}') \n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#optimizer_forest = optim.Adam(forest_net.parameters(), lr = 0.001)\n",
    "#optimizer_forest = optim.SGD(forest_net.parameters(), lr = 0.001, momentum = 0.5)#, weight_decay = 0.0001)\n",
    "optimizer_forest = optim.AdamW(forest_net.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer_forest, step_size = 50, gamma = 0.1, verbose = True)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_forest, verbose = True)\n",
    "\n",
    "save_best_model = SaveBestModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22571757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Training \n",
    "\n",
    "forest_train_loss = []\n",
    "forest_val_loss = []\n",
    "forest_val_acc = []\n",
    "forest_net.to(device)\n",
    "    \n",
    "tqdm_bar = tqdm(range(1, num_epochs+1), desc=\"epoch [loss: ...]\")\n",
    "for epoch in tqdm_bar:\n",
    "    \n",
    "    forest_net.train()\n",
    "    train_loss_averager = make_averager()\n",
    "    \n",
    "    batch_bar =  tqdm(forest_train, leave=False, desc='batch', total=len(forest_train))\n",
    "    for tr_inputs, tr_classes in batch_bar:\n",
    "        \n",
    "        loss = Train(optimizer_forest, tr_inputs, tr_classes, device, forest_net, loss_func, rand)\n",
    "        refresh_bar(batch_bar, f\"train batch [loss: {train_loss_averager(loss.item()):.3f}]\")\n",
    "        \n",
    "    forest_train_loss.append(train_loss_averager(None))\n",
    "    refresh_bar(tqdm_bar, f\"epoch [loss: {train_loss_averager(None):.3f}]\")\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    forest_net.eval()\n",
    "    sum_misclassified = 0\n",
    "    val_loss_averager = make_averager()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, classes in forest_valid:\n",
    "            \n",
    "            loss, misclassified = Validation(inputs, classes, device, forest_net, loss_func)\n",
    "            sum_misclassified += misclassified\n",
    "            val_loss_averager(loss.item())\n",
    "            \n",
    "        val_acc = 1 - sum_misclassified/forest_len_val\n",
    "        forest_val_loss.append(val_loss_averager(None))\n",
    "        forest_val_acc.append(val_acc)\n",
    "    \n",
    "    save_best_model(forest_val_loss[-1], epoch, forest_net, optimizer_forest, loss_func, forest_train_loss[-1], forest_val_acc[-1])\n",
    "    #scheduler.step()\n",
    "    #lr_schedule(epoch)\n",
    "    \n",
    "    print(f'Epoch: {epoch} \\t Training loss: {forest_train_loss[-1]:.3f} \\tValidation loss: {forest_val_loss[-1]:.3f} \\tValidation accuracy: {val_acc:.3f}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_train_loss, title= \"Train loss\", axis = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_val_loss, title= \"Validation loss\", axis = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27817117",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_val_acc, title= \"Validation\", axis = \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56c11b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/3632 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.12334448099136353 \tTest Loss: 7.876975824982584\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "forest_net.eval()\n",
    "test_loss_averager = make_averager()\n",
    "sum_misclassified = 0\n",
    "forest_net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_bar =  tqdm(forest_valid, leave=False, desc='batch', total=len(forest_valid))\n",
    "    for inputs, classes in batch_bar:\n",
    "        loss, misclassified = Validation(inputs, classes, device, forest_net, loss_func)\n",
    "        sum_misclassified += misclassified\n",
    "        test_loss_averager(loss.item())\n",
    "\n",
    "    test_acc = 1 - sum_misclassified/forest_len_test\n",
    "    forest_test_loss = test_loss_averager(None)\n",
    "print(f\"Accuracy: {test_acc} \\tTest Loss: {forest_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296556e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '24'\n",
    "with open(f'train_loss{name}.pkl', 'wb') as f:\n",
    "    pickle.dump(ConvList(forest_train_loss), f)\n",
    "\n",
    "with open(f'val_loss{name}.pkl', 'wb') as f1:\n",
    "    pickle.dump(ConvList(forest_val_loss), f1)\n",
    "\n",
    "with open(f'val_acc{name}.pkl', 'wb') as f2:\n",
    "    pickle.dump(ConvList(forest_val_acc), f2)\n",
    "\n",
    "#with open(f'test{name}.pkl', 'wb') as f3:\n",
    " #   pickle.dump(ConvList([test_loss, test_acc] + y_true.to('cpu') + y_score.to('cpu')), f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056454bc",
   "metadata": {},
   "source": [
    "## Only the BN parameters are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e08906",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_net = MLP_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e4999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin1.weight True\n",
      "lin2.weight True\n",
      "lin3.weight True\n",
      "lin4.weight True\n",
      "linout.weight True\n",
      "bn1.weight True\n",
      "bn1.bias True\n",
      "bn1.running_mean True\n",
      "bn1.running_var True\n",
      "bn1.num_batches_tracked True\n",
      "bn2.weight True\n",
      "bn2.bias True\n",
      "bn2.running_mean True\n",
      "bn2.running_var True\n",
      "bn2.num_batches_tracked True\n",
      "bn3.weight True\n",
      "bn3.bias True\n",
      "bn3.running_mean True\n",
      "bn3.running_var True\n",
      "bn3.num_batches_tracked True\n",
      "bn4.weight True\n",
      "bn4.bias True\n",
      "bn4.running_mean True\n",
      "bn4.running_var True\n",
      "bn4.num_batches_tracked True\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights and check if it is done correctly\n",
    "InitializeWeight(forest_net)\n",
    "HaveSameWeight(forest_net, 'weight_initialization.pth', 'cpu', is_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42715d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only the BN parameters are trainable \n",
    "for param in forest_net.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in forest_net.bn1.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in forest_net.bn2.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in forest_net.bn3.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in forest_net.bn4.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2509a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check that all the paramters are trainable \n",
    "for param in forest_net.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b44f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "rand = False # Set to True when we train the random weights network\n",
    "forest_len_val = forest_y_valid.shape[0]\n",
    "forest_len_train = forest_y_train.shape[0]\n",
    "forest_len_test = forest_y_test.shape[0]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}') \n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#optimizer_forest = optim.Adam(forest_net.parameters(), lr = 0.001)\n",
    "#optimizer_forest = optim.SGD(forest_net.parameters(), lr = 0.001, momentum = 0.5)#, weight_decay = 0.0001)\n",
    "optimizer_forest = optim.AdamW(forest_net.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer_forest, step_size = 50, gamma = 0.1, verbose = True)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_forest, verbose = True)\n",
    "\n",
    "save_best_model = SaveBestModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376fdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Training\n",
    "\n",
    "forest_train_loss = []\n",
    "forest_val_loss = []\n",
    "forest_val_acc = []\n",
    "forest_net.to(device)\n",
    "\n",
    "tqdm_bar = tqdm(range(1, num_epochs+1), desc=\"epoch [loss: ...]\")\n",
    "for epoch in tqdm_bar:\n",
    "\n",
    "    forest_net.train()\n",
    "    train_loss_averager = make_averager()\n",
    "\n",
    "    batch_bar =  tqdm(forest_train, leave=False, desc='batch', total=len(forest_train))\n",
    "    for tr_inputs, tr_classes in batch_bar:\n",
    "\n",
    "        loss = Train(optimizer_forest, tr_inputs, tr_classes, device, forest_net, loss_func, rand)\n",
    "        refresh_bar(batch_bar, f\"train batch [loss: {train_loss_averager(loss.item()):.3f}]\")\n",
    "\n",
    "    forest_train_loss.append(train_loss_averager(None))\n",
    "    refresh_bar(tqdm_bar, f\"epoch [loss: {train_loss_averager(None):.3f}]\")\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    forest_net.eval()\n",
    "    sum_misclassified = 0\n",
    "    val_loss_averager = make_averager()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, classes in forest_valid:\n",
    "\n",
    "            loss, misclassified = Validation(inputs, classes, device, forest_net, loss_func)\n",
    "            sum_misclassified += misclassified\n",
    "            val_loss_averager(loss.item())\n",
    "\n",
    "        val_acc = 1 - sum_misclassified/forest_len_val\n",
    "        forest_val_loss.append(val_loss_averager(None))\n",
    "        forest_val_acc.append(val_acc)\n",
    "\n",
    "    save_best_model(forest_val_loss[-1], epoch, forest_net, optimizer_forest, loss_func, forest_train_loss[-1], forest_val_acc[-1])\n",
    "    scheduler.step()\n",
    "    #lr_schedule(epoch)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\t Training loss: {forest_train_loss[-1]:.3f} \\tValidation loss: {forest_val_loss[-1]:.3f} \\tValidation accuracy: {val_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc28c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_train_loss, title= \"Train loss\", axis = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dace03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_val_loss, title= \"Validation loss\", axis = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeacb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_val_acc, title= \"Validation\", axis = \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "forest_net.eval()\n",
    "test_loss_averager = make_averager()\n",
    "sum_misclassified = 0\n",
    "forest_net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_bar =  tqdm(forest_valid, leave=False, desc='batch', total=len(forest_valid))\n",
    "    for inputs, classes in batch_bar:\n",
    "        loss, misclassified = Validation(inputs, classes, device, forest_net, loss_func)\n",
    "        sum_misclassified += misclassified\n",
    "        test_loss_averager(loss.item())\n",
    "\n",
    "    test_acc = 1 - sum_misclassified/forest_len_test\n",
    "    forest_test_loss = test_loss_averager(None)\n",
    "print(f\"Accuracy: {test_acc} \\tTest Loss: {forest_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvList(l):\n",
    "    # It puts the list l from CUDA to CPU\n",
    "    return list(torch.Tensor(l).to('cpu'))\n",
    "\n",
    "name = 'BN19'\n",
    "with open(f'train_loss{name}.pkl', 'wb') as f:\n",
    "    pickle.dump(ConvList(forest_train_loss), f)\n",
    "\n",
    "with open(f'val_loss{name}.pkl', 'wb') as f1:\n",
    "    pickle.dump(ConvList(forest_val_loss), f1)\n",
    "\n",
    "with open(f'val_acc{name}.pkl', 'wb') as f2:\n",
    "    pickle.dump(ConvList(forest_val_acc), f2)\n",
    "\n",
    "#with open(f'test{name}.pkl', 'wb') as f3:\n",
    " #   pickle.dump(ConvList([test_loss, test_acc] + y_true.to('cpu') + y_score.to('cpu')), f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d0823",
   "metadata": {},
   "source": [
    "## Only some random wieghts are trainable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2ad792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin1.weight True\n",
      "lin2.weight True\n",
      "lin3.weight True\n",
      "lin4.weight True\n",
      "linout.weight True\n",
      "bn1.weight True\n",
      "bn1.bias True\n",
      "bn1.running_mean True\n",
      "bn1.running_var True\n",
      "bn1.num_batches_tracked True\n",
      "bn2.weight True\n",
      "bn2.bias True\n",
      "bn2.running_mean True\n",
      "bn2.running_var True\n",
      "bn2.num_batches_tracked True\n",
      "bn3.weight True\n",
      "bn3.bias True\n",
      "bn3.running_mean True\n",
      "bn3.running_var True\n",
      "bn3.num_batches_tracked True\n",
      "bn4.weight True\n",
      "bn4.bias True\n",
      "bn4.running_mean True\n",
      "bn4.running_var True\n",
      "bn4.num_batches_tracked True\n"
     ]
    }
   ],
   "source": [
    "# Define the net initiliaze and check the initialization\n",
    "forest_net = MLP_forest_RandWeights()\n",
    "InitializeCheckRandWieght(forest_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2721d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters except the random weights\n",
    "\n",
    "for param in forest_net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "forest_net.lin1.tr_weights.requires_grad = True\n",
    "forest_net.lin2.tr_weights.requires_grad = True\n",
    "forest_net.lin3.tr_weights.requires_grad = True\n",
    "forest_net.lin4.tr_weights.requires_grad = True\n",
    "forest_net.linout.tr_weights.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9746f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check which are freezed\n",
    "for param in forest_net.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefae4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "rand = True\n",
    "forest_len_val = forest_y_valid.shape[0]\n",
    "forest_len_train = forest_y_train.shape[0]\n",
    "forest_len_test = forest_y_test.shape[0]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer_forest = optim.Adam(forest_net.parameters(), lr = 0.001)\n",
    "#optimizer_forest = optim.SGD(forest_net.parameters(), lr = 0.001, momentum = 0.5)#, weight_decay = 0.0001)\n",
    "#optimizer_forest = optim.AdamW(forest_net.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_forest, step_size = 50, gamma = 0.1, verbose = True)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_forest, verbose = True)\n",
    "\n",
    "save_best_model = SaveBestModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e18fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Training\n",
    "\n",
    "forest_train_loss = []\n",
    "forest_val_loss = []\n",
    "forest_val_acc = []\n",
    "forest_net.to(device)\n",
    "\n",
    "tqdm_bar = tqdm(range(1, num_epochs+1), desc=\"epoch [loss: ...]\")\n",
    "for epoch in tqdm_bar:\n",
    "\n",
    "    forest_net.train()\n",
    "    train_loss_averager = make_averager()\n",
    "\n",
    "    batch_bar =  tqdm(forest_train, leave=False, desc='batch', total=len(forest_train))\n",
    "    for tr_inputs, tr_classes in batch_bar:\n",
    "\n",
    "        loss = Train(optimizer_forest, tr_inputs, tr_classes, device, forest_net, loss_func, rand)\n",
    "        refresh_bar(batch_bar, f\"train batch [loss: {train_loss_averager(loss.item()):.3f}]\")\n",
    "\n",
    "    forest_train_loss.append(train_loss_averager(None))\n",
    "    refresh_bar(tqdm_bar, f\"epoch [loss: {train_loss_averager(None):.3f}]\")\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    forest_net.eval()\n",
    "    sum_misclassified = 0\n",
    "    val_loss_averager = make_averager()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, classes in forest_valid:\n",
    "\n",
    "            loss, misclassified = Validation(inputs, classes, device, forest_net, loss_func)\n",
    "            sum_misclassified += misclassified\n",
    "            val_loss_averager(loss.item())\n",
    "\n",
    "        val_acc = 1 - sum_misclassified/forest_len_val\n",
    "        forest_val_loss.append(val_loss_averager(None))\n",
    "        forest_val_acc.append(val_acc)\n",
    "\n",
    "    save_best_model(forest_val_loss[-1], epoch, forest_net, optimizer_forest, loss_func, forest_train_loss[-1], forest_val_acc[-1])\n",
    "    scheduler.step()\n",
    "    #lr_schedule(epoch)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\t Training loss: {forest_train_loss[-1]:.3f} \\tValidation loss: {forest_val_loss[-1]:.3f} \\tValidation accuracy: {val_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ae3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_train_loss, title= \"Train loss\", axis = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c98bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_val_loss, title= \"Validation loss\", axis = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(forest_val_acc, title= \"Validation\", axis = \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f58a4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/3632 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8092734217643738 \tTest Loss: 0.4442348908386829\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "forest_net.eval()\n",
    "test_loss_averager = make_averager()\n",
    "sum_misclassified = 0\n",
    "forest_net.to(device)\n",
    "#y_true = torch.Tensor([]).to(device)\n",
    "#y_score = torch.Tensor([]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_bar =  tqdm(forest_valid, leave=False, desc='batch', total=len(forest_valid))\n",
    "    for inputs, classes in batch_bar:\n",
    "        loss, misclassified = Validation(inputs, classes, device, forest_net, loss_func)\n",
    "        sum_misclassified += misclassified\n",
    "        test_loss_averager(loss.item())\n",
    "        \n",
    "        #Stuff to compute the ROC curve\n",
    "        #cl = torch.clamp(classes[:,0]-classes[:,1], min =0, max = 1 )\n",
    "        #y_true = torch.cat((y_true, cl), dim = 0)\n",
    "        #pred = net(inputs)\n",
    "        #pr = torch.clamp(pred[:,0]-pred[:,1], min =0, max = 1 )\n",
    "        #y_score = torch.cat((y_score, pr), dim = 0)\n",
    "\n",
    "    test_acc = 1 - sum_misclassified/forest_len_test\n",
    "    forest_test_loss = test_loss_averager(None)\n",
    "print(f\"Accuracy: {test_acc} \\tTest Loss: {forest_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvList(l):\n",
    "    # It puts the list l from CUDA to CPU\n",
    "    return list(torch.Tensor(l).to('cpu'))\n",
    "\n",
    "name = 'Rand4'\n",
    "with open(f'train_loss{name}.pkl', 'wb') as f:\n",
    "    pickle.dump(ConvList(forest_train_loss), f)\n",
    "\n",
    "with open(f'val_loss{name}.pkl', 'wb') as f1:\n",
    "    pickle.dump(ConvList(forest_val_loss), f1)\n",
    "\n",
    "with open(f'val_acc{name}.pkl', 'wb') as f2:\n",
    "    pickle.dump(ConvList(forest_val_acc), f2)\n",
    "\n",
    "#with open(f'test{name}.pkl', 'wb') as f3:\n",
    " #   pickle.dump(ConvList([test_loss, test_acc] + y_true.to('cpu') + y_score.to('cpu')), f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f41707",
   "metadata": {},
   "source": [
    "# Control and compare architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d87e09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "for name in ['18', 'BN7', 'Rand4' ]:\n",
    "    with open(f'train_loss{name}.pkl', 'rb') as f:\n",
    "        train_loss.append(pickle.load(f))\n",
    "    \n",
    "    with open(f'val_loss{name}.pkl', 'rb') as f1:\n",
    "        val_loss.append(pickle.load(f1))\n",
    "        \n",
    "    with open(f'val_acc{name}.pkl', 'rb') as f2:\n",
    "        val_acc.append(pickle.load(f2))\n",
    "    \n",
    "    #with open(f'test{name}.pkl', 'rb') as f3:\n",
    "    #    [test_loss, test_acc] = pickle.load(f3)\n",
    "    #    \n",
    "    #with open(f'test_true{name}.pkl', 'rb') as f3:\n",
    "    #    y_true.append(pickle.load(f3))\n",
    "    #\n",
    "    #with open(f'test_score{name}.pkl', 'rb') as f3:\n",
    "    #    y_score.append(pickle.load(f3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8191e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_nets(train_loss, name_title = 'Train Loss', name_axis = 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22162e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_nets(val_loss, name_title = 'Validation Loss', name_axis = 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_nets(val_acc, name_title = 'Validation Accuracy', name_axis = 'Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ade54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC_nets(y_true, y_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f672000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title import dependencies\n",
    "\n",
    "from typing import Mapping, Union, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import Callable, Optional\n",
    "from torchsummary import summary\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(0)\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecca22",
   "metadata": {},
   "source": [
    "# Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304c6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it on the global dataset \n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.target_names = data['target_names']\n",
    "        self.feature_names = data['feature_names']\n",
    "        self.data = data['data']\n",
    "        self.target = data['target']    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        obj = self.data[idx]\n",
    "        label = self.target[idx]\n",
    "        return obj, label\n",
    "    \n",
    "# Use it on the single subsets (train, test, validation)\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, x,y,data):\n",
    "        super().__init__()\n",
    "        self.target_names = data['target_names']\n",
    "        self.feature_names = data['feature_names']\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obj = self.data[idx]\n",
    "        label = self.target[idx]\n",
    "        return obj, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb63d7",
   "metadata": {},
   "source": [
    "# Architecture (net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403b28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_iris (nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() \n",
    "        self.lin1 = nn.Linear(4,32, bias = False)\n",
    "        self.lin2 = nn.Linear(32,16, bias = False)\n",
    "        self.linout = nn.Linear(16,3, bias = False) \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        self.acthid = nn.ReLU()\n",
    "        self.actout = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.acthid(self.bn1(self.lin1(x)))\n",
    "        x = self.acthid(self.bn2(self.lin2(x)))\n",
    "        x = self.linout(x)\n",
    "        return x #self.actout(x) # when we don't use CE, cause it has a built in sigmoid\n",
    "\n",
    "class MLP_forest (nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() \n",
    "        self.lin1 = nn.Linear(54,500, bias = False)\n",
    "        self.lin2 = nn.Linear(500,300, bias = False)\n",
    "        self.lin3 = nn.Linear(300,150, bias = False)\n",
    "        self.lin4 = nn.Linear(150,70, bias = False)\n",
    "        self.linout = nn.Linear(70,7, bias = False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(500)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.bn3 = nn.BatchNorm1d(150)\n",
    "        self.bn4 = nn.BatchNorm1d(70)\n",
    "        \n",
    "        self.acthid = nn.ReLU()\n",
    "        self.actout = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.acthid(self.bn1(self.lin1(x)))\n",
    "        x = self.acthid(self.bn2(self.lin2(x)))\n",
    "        x = self.acthid(self.bn3(self.lin3(x)))\n",
    "        x = self.acthid(self.bn4(self.lin4(x)))\n",
    "        x = self.linout(x)\n",
    "        return x  #self.actout(x) # when we don't use CE\n",
    "\n",
    "    \n",
    "class MLP_forest_noBN (nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() \n",
    "        self.lin1 = nn.Linear(54,500, bias = False)\n",
    "        self.lin2 = nn.Linear(500,300, bias = False)\n",
    "        self.lin3 = nn.Linear(300,150, bias = False)\n",
    "        self.lin4 = nn.Linear(150,70, bias = False)\n",
    "        self.linout = nn.Linear(70,7, bias = False)\n",
    "        \n",
    "        self.acthid = nn.ReLU()\n",
    "        self.actout = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.acthid(self.lin1(x))\n",
    "        x = self.acthid(self.lin2(x))\n",
    "        x = self.acthid(self.lin3(x))\n",
    "        x = self.acthid(self.lin4(x))\n",
    "        x = self.linout(x)\n",
    "        return x  #self.actout(x) # when we don't use CE\n",
    "    \n",
    "class MyLinearLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer\n",
    "        so we can select just some sparse random weight and set them to be trainable \"\"\"\n",
    "    def __init__(self, size_in, size_out, num_layer):\n",
    "        # num_layer goes from 1 to 5 depending on the layer that comes before this one\n",
    "        super().__init__()\n",
    "\n",
    "        memory, num_weight_lin = torch.load('memory.pth')\n",
    "        self.start = 0\n",
    "        for i in range(num_layer-1):\n",
    "            self.start += num_weight_lin[i]\n",
    "        self.num_weights = num_weight_lin[num_layer-1]\n",
    "        self.index = memory[self.start : self.start + self.num_weights] # Indices of the randomly selected trainable parameters\n",
    "\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        self.weight = nn.Parameter(weights)  # Parameters that will be set to untrainable\n",
    "        self.tr_weights = nn.Parameter(torch.Tensor(self.num_weights)) # Trainable parameters\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "         # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5)) # weight init\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / np.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.tr_weights, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        w1 = torch.zeros(self.size_out,self.size_in).to(self.device)\n",
    "        w1[self.index[:,0], self.index[:,1]] = self.tr_weights\n",
    "        w_times_x= torch.mm(x, (self.weight + w1).t())\n",
    "        return w_times_x\n",
    "    \n",
    "class MLP_forest_RandWeights (nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() \n",
    "        self.lin1 = MyLinearLayer(54,500, 1)\n",
    "        self.lin2 = MyLinearLayer(500,300, 2)\n",
    "        self.lin3 = MyLinearLayer(300,150, 3)\n",
    "        self.lin4 = MyLinearLayer(150,70, 4)\n",
    "        self.linout = MyLinearLayer(70,7, 5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(500)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.bn3 = nn.BatchNorm1d(150)\n",
    "        self.bn4 = nn.BatchNorm1d(70)\n",
    "        \n",
    "        self.acthid = nn.ReLU()\n",
    "        self.actout = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.acthid(self.bn1(self.lin1(x)))\n",
    "        x = self.acthid(self.bn2(self.lin2(x)))\n",
    "        x = self.acthid(self.bn3(self.lin3(x)))\n",
    "        x = self.acthid(self.bn4(self.lin4(x)))\n",
    "        x = self.linout(x)\n",
    "        return x  #self.actout(x) # when we don't use CE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600253e",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c7c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_averager() -> Callable[[Optional[float]], float]:\n",
    "    \"\"\" Returns a function that maintains a running average\n",
    "\n",
    "    :returns: running average function\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    total = 0\n",
    "\n",
    "    def averager(new_value: Optional[float]) -> float:\n",
    "        \"\"\" Running averager\n",
    "\n",
    "        :param new_value: number to add to the running average,\n",
    "                          if None returns the current average\n",
    "        :returns: the current average\n",
    "        \"\"\"\n",
    "        nonlocal count, total\n",
    "        if new_value is None:\n",
    "            return total / count if count else float(\"nan\")\n",
    "        count += 1\n",
    "        total += new_value\n",
    "        return total / count\n",
    "\n",
    "    return averager\n",
    "\n",
    "def refresh_bar(bar, desc):\n",
    "    bar.set_description(desc)\n",
    "    bar.refresh()\n",
    "\n",
    "def plot_loss(losses, title= \"Train loss\", axis = \"Loss\"):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(losses))),\n",
    "        y=losses,\n",
    "        # name=\"Name of Trace 1\"       # this sets its legend entry\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=axis,\n",
    "        font=dict(\n",
    "            family=\"Courier New, monospace\",\n",
    "            size=18,\n",
    "            color=\"#7f7f7f\"\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "class SaveBestModel:\n",
    "    def __init__(self, best_valid_loss=float('inf'), last_model_train_loss=float('inf')): #object initialized with best_loss = +infinite\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.last_model_train_loss = last_model_train_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model, optimizer, criterion, loss, acc,\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss and loss < self.last_model_train_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            self.last_model_train_loss = loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            # method to save a model (the state_dict: a python dictionary object that \n",
    "            # maps each layer to its parameter tensor) and other useful parametrers\n",
    "            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                'train_loss_value': loss,\n",
    "                'val_loss_value' : current_valid_loss,\n",
    "                'accuracy': acc,\n",
    "                }, 'best_model.pth')\n",
    "\n",
    "def Train(opt, inputs, classes, device, net, loss_func, rand = False):\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    classes = (classes.to(device)-1).long() # I put -1 cause the classes are definied in the interval [1;7] while I need [0;6]\n",
    "\n",
    "    opt.zero_grad()\n",
    "    pred = net(inputs)\n",
    "    loss = loss_func(pred, classes)\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    #if rand:\n",
    "     #   with torch.no_grad():\n",
    "            #RandomWeights(net, device)\n",
    "    return loss\n",
    "    \n",
    "def Validation(inputs, classes, device, net, loss_func):    \n",
    "    inputs = inputs.to(device)\n",
    "    classes = (classes.to(device)-1).long() # I put -1 cause the classes are definied in the interval [1;7] while I need [0;6]\n",
    "        \n",
    "    pred = net(inputs)\n",
    "    loss = loss_func(pred, classes)\n",
    "    _, pred = torch.max(pred, 1)\n",
    "    missclassified = torch.sum(torch.clamp(torch.abs(pred-classes), 0,1))\n",
    "    \n",
    "    return loss, missclassified\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 10:\n",
    "        lr *= 0.1\n",
    "    elif epoch > 20:\n",
    "        lr *= 0.01\n",
    "    return lr\n",
    "\n",
    "def InitializeWeight (rete, file = 'weight_initialization.pth', is_dict = True):\n",
    "    # Load the weight in file to forest_net and checks that this is done successfully\n",
    "    init_weight_dict = torch.load(file)\n",
    "    if not is_dict:\n",
    "        init_weight_dict = init_weight_dict['model_state_dict']\n",
    "    \n",
    "    # Overwrite the weights\n",
    "    with torch.no_grad():\n",
    "        rete.load_state_dict(init_weight_dict)\n",
    "\n",
    "def HaveSameWeight(rete, file, device, is_dict=True):\n",
    "    # Check if the weights of the two nets are equal\n",
    "    \n",
    "    init_weight_dict = torch.load(file)\n",
    "    if not is_dict:\n",
    "        init_weight_dict = init_weight_dict['model_state_dict']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in rete.state_dict().keys():\n",
    "            j = all((rete.state_dict()[i].to(device) == init_weight_dict[i].to(device)).detach().numpy().reshape(-1))\n",
    "            print(i,j)\n",
    "\n",
    "# Initial attempt to select just some random weights\n",
    "def RandomWeights(forest_net, device):\n",
    "    # Reset the values of all the weights that are not trainable\n",
    "    memory, num_weight_layers = torch.load('memory.pth')\n",
    "    saved_weights = torch.Tensor(2040)\n",
    "    res = 0\n",
    "    layers = ['1','2','3','4','out']\n",
    "    \n",
    "    #Save the current value of the trainable weights\n",
    "    for i, num in enumerate(num_weight_layers):\n",
    "        saved_weights[res : res+num] = forest_net.state_dict()[f'lin{layers[i]}.weight'][memory[res : res+num,0], memory[res : res+num,1]]\n",
    "        res += num\n",
    "    \n",
    "    #Reset all the weights (trainable and not) to their initial value and check this\n",
    "    InitializeWeight(forest_net)\n",
    "    #HaveSameWeight(forest_net, 'weight_initialization.pth', 'cpu', is_dict=True)\n",
    "    \n",
    "    #Reset the trainable weights to the value after the training (so the updated one) \n",
    "    res = 0\n",
    "    for i, num in enumerate(num_weight_layers):\n",
    "        forest_net.state_dict()[f'lin{layers[i]}.weight'][memory[res : res+num,0], memory[res : res+num,1]] = saved_weights[res : res+num].to(device)\n",
    "        res += num\n",
    "    \n",
    "    #Check that this update is successful\n",
    "    #HaveSameWeight(forest_net, 'weight_initialization.pth', 'cpu', is_dict=True)\n",
    "\n",
    "def ConvList(l):\n",
    "    # It puts the list l from CUDA to CPU\n",
    "    return list(torch.Tensor(l).to('cpu'))\n",
    "\n",
    "def InitializeCheckRandWieght(rete):\n",
    "    wi = torch.load('weight_initialization.pth')\n",
    "\n",
    "    rete.lin1.weight = nn.Parameter(wi['lin1.weight'])\n",
    "    rete.lin2.weight = nn.Parameter(wi['lin2.weight'])\n",
    "    rete.lin3.weight = nn.Parameter(wi['lin3.weight'])\n",
    "    rete.lin4.weight = nn.Parameter(wi['lin4.weight'])\n",
    "    rete.linout.weight = nn.Parameter(wi['linout.weight'])\n",
    "\n",
    "    for i in wi:\n",
    "            j = all((rete.state_dict()[i] == wi[i]).detach().numpy().reshape(-1))\n",
    "            print(i,j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d62dc",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e68549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueModel(file, train, val, test, device, loss_func, len_train, len_val, len_test, net = MLP_forest()):\n",
    "    # Given the model's weight in file it loads it in the net and then evaluate this in the different dataset\n",
    "    # file is the name of the file where there is the dict saved with save_best_model. Il file deve stare in una cartella chiamata BestModels che si trova dove siamo\n",
    "    # train, val, test are the dataloader of the corresponding sets\n",
    "    #%cd BestModels\n",
    "    InitializeWeight(net, file, is_dict = False)\n",
    "    with torch.no_grad():\n",
    "        loss_avg = make_averager()\n",
    "        miss_sum = 0\n",
    "        batch_bar =  tqdm(train, leave=False, desc='train', total=len(train))\n",
    "        for inputs, classes in batch_bar:\n",
    "            loss, miss = Validation(inputs, classes, device, net, loss_func)\n",
    "            miss_sum += miss\n",
    "            loss_avg(loss.item())\n",
    "        tr_acc = 1 - miss_sum/len_train\n",
    "        tr_loss = loss_avg(None)\n",
    "\n",
    "        loss_avg = make_averager()\n",
    "        miss_sum = 0\n",
    "        batch_bar =  tqdm(val, leave=False, desc='validation', total=len(val))\n",
    "        for inputs, classes in batch_bar:\n",
    "            loss, miss = Validation(inputs, classes, device, net, loss_func)\n",
    "            miss_sum += miss\n",
    "            loss_avg(loss.item())\n",
    "        val_acc = 1 - miss_sum/len_val\n",
    "        val_loss = loss_avg(None)\n",
    "        \n",
    "        loss_avg = make_averager()\n",
    "        miss_sum = 0\n",
    "        batch_bar =  tqdm(test, leave=False, desc='test', total=len(test))\n",
    "        for inputs, classes in batch_bar:\n",
    "            loss, miss = Validation(inputs, classes, device, net, loss_func)\n",
    "            miss_sum += miss\n",
    "            loss_avg(loss.item())\n",
    "        test_acc = 1 - miss_sum/len_test\n",
    "        test_loss = loss_avg(None)\n",
    "    \n",
    "    print(f'Model loss and acc: Train: {tr_loss:.4f}     {tr_acc:.4f} \\tVal: {val_loss:.4f}      {val_acc:.4f} \\tTest: {test_loss:.4f}     {test_acc:.4f}')\n",
    "    #%cd ..\n",
    "    return (tr_loss, tr_acc), (val_loss, val_acc), (test_loss, test_acc)\n",
    "\n",
    "def ShowTrainFile(name):\n",
    "    \n",
    "    with open(f'train_loss{name}.pkl', 'rb') as f:\n",
    "        train_loss = pickle.load(f)\n",
    "    \n",
    "    with open(f'val_loss{name}.pkl', 'rb') as f1:\n",
    "        val_loss = pickle.load(f1)\n",
    "        \n",
    "    with open(f'val_acc{name}.pkl', 'rb') as f2:\n",
    "        val_acc = pickle.load(f2)\n",
    "    a = len(val_loss)\n",
    "    if len(train_loss) == a and len(val_acc) == a:\n",
    "        print(f'The training phase of the model {name} is:\\n')\n",
    "        for i in range(a):\n",
    "            print(f'Epochs: {i} \\tTrain Loss: {train_loss[i]:.4f} \\tValidation Loss: {val_loss[i]:.4f} \\tValidation Accuracy: {val_acc[i]:.4f}\\n')\n",
    "            \n",
    "def ShowTrain(train_loss,val_loss,val_acc):\n",
    "    a = len(val_loss)\n",
    "    if len(train_loss) == a and len(val_acc) == a:\n",
    "        print(f'The training phase of the model is:\\n')\n",
    "        for i in range(a):\n",
    "            print(f'Epochs: {i} \\tTrain Loss: {train_loss[i]:.4f} \\tValidation Loss: {val_loss[i]:.4f} \\tValidation Accuracy: {val_acc[i]:.4f}')\n",
    "\n",
    "        \n",
    "def ROC_nets(truth, score):\n",
    "    titles = ['Particle Net Lite', 'Particle Net Very Lite ch3', 'Particle Net Very Lite ch2']\n",
    "    \n",
    "    for i in range(len(truth)):\n",
    "        fpr, tpr, _ = roc_curve(truth[i], score[i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label = ('AUC = %0.2f of the ' % roc_auc)+ titles[i])\n",
    "\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_nets(losses, name_title = 'Train loss', name_axis = 'Loss'):\n",
    "    # losses: list of list\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    titles = ['Train', 'BN', 'Rand']\n",
    "    for i,los in enumerate(losses):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(los))),\n",
    "            y=los,\n",
    "            name= titles[i]\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title= name_title + ' for all the trained networks',\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=name_axis,\n",
    "        font=dict(\n",
    "            family=\"Courier New, monospace\",\n",
    "            size=18,\n",
    "            color=\"#7f7f7f\"\n",
    "        )\n",
    "    )\n",
    "    return fig\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
